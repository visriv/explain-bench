# ğŸ§  ExplainBench: Benchmarking XAI Methods on Time-Series Data

ExplainBench is a **modular and extensible benchmarking framework** for evaluating explainability methods on **time series models** across diverse datasets and metrics.

## ğŸš€ Features

- **Extensible**: Add new datasets, models, explainers, metrics via a simple registry.
- **Reproducible**: YAML-configured runs, fixed seeds, deterministic loaders where possible.
- **Visual**: Attribution plots and tabular reports.
- **Batteries included**: Synthetic dataset, LSTM & Transformer baselines, Grad/IG/LIME explainers, Faithfulness/Consistency/Stability metrics.

## ğŸ—‚ï¸ Structure

```
explain-bench/
â”œâ”€â”€ explainbench/
â”‚   â”œâ”€â”€ configs/          # YAML configs
â”‚   â”œâ”€â”€ datasets/         # loaders: synthetic, UCR (stub)
â”‚   â”œâ”€â”€ models/           # LSTM, Transformer
â”‚   â”œâ”€â”€ explanations/     # Grad, IG, LIME (simple)
â”‚   â”œâ”€â”€ metrics/          # Faithfulness, Consistency, Stability
â”‚   â”œâ”€â”€ visualization/    # plotting & reporting
â”‚   â”œâ”€â”€ utils/            # registry, config, logging
â”‚   â”œâ”€â”€ benchmark.py      # core benchmark loop
â”‚   â”œâ”€â”€ runner.py         # orchestrator
â”œâ”€â”€ scripts/              # CLI scripts
â”œâ”€â”€ tests/                # basic sanity tests
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ setup.py
```

## âš™ï¸ Install

```bash
pip install -e .
# or
pip install -r requirements.txt && python setup.py develop
```

## ğŸ§  Quickstart

```bash
python scripts/run_benchmark.py --config explainbench/configs/default.yaml
python scripts/visualize_results.py --input results/benchmark_results.csv
```

## ğŸ§± Add New Components

See docstrings in `explainbench/utils/registry.py` and base classes in each submodule.

## ğŸ“œ License

MIT
